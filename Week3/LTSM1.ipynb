{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n\u001b[1mDownloading and preparing dataset imdb_reviews/subwords8k/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /home/sahanaka/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\nDl Completed...: 0 url [00:00, ? url/s]\nDl Size...: 0 MiB [00:00, ? MiB/s]\nDl Completed...: 0 url [00:00, ? url/s]\n0 examples [00:00, ? examples/s]\n\n  2%|▏         | 521/25000 [00:00<00:04, 5209.51 examples/s]Shuffling and writing examples to /home/sahanaka/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete4CFP5P/imdb_reviews-train.tfrecord\n 14%|█▍        | 3606/25000 [00:00<00:00, 36056.77 examples/s]Shuffling and writing examples to /home/sahanaka/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete4CFP5P/imdb_reviews-test.tfrecord\n  0%|          | 1/50000 [00:00<1:40:07,  8.32 examples/s]Shuffling and writing examples to /home/sahanaka/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete4CFP5P/imdb_reviews-unsupervised.tfrecord\n 81%|████████▏ | 40724/50000 [00:00<09:06, 16.98 examples/s]\u001b[1mDataset imdb_reviews downloaded and prepared to /home/sahanaka/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
    }
   ],
   "source": [
    "# Get the data\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of the model using one LSTM layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, None, 64)          523840    \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               66048     \n_________________________________________________________________\ndense (Dense)                (None, 64)                8256      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 598,209\nTrainable params: 598,209\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n391/391 [==============================] - 727s 2s/step - loss: 0.5411 - accuracy: 0.7306 - val_loss: 0.4871 - val_accuracy: 0.7705\nEpoch 2/2\n391/391 [==============================] - 511s 1s/step - loss: 0.4041 - accuracy: 0.8350 - val_loss: 0.4811 - val_accuracy: 0.7808\n"
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "history = model.fit(train_dataset, epochs = NUM_EPOCHS, validation_data = (test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with multilayer LSTM\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences = True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, None, 64)          523840    \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, None, 128)         66048     \n_________________________________________________________________\nbidirectional_4 (Bidirection (None, 64)                41216     \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 635,329\nTrainable params: 635,329\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n391/391 [==============================] - 1189s 3s/step - loss: 0.6457 - accuracy: 0.6181 - val_loss: 0.6339 - val_accuracy: 0.6568\nEpoch 2/2\n391/391 [==============================] - 960s 2s/step - loss: 0.5333 - accuracy: 0.7342 - val_loss: 0.4138 - val_accuracy: 0.8121\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f41a05ddd68>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "model2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model2.fit(train_dataset, epochs = NUM_EPOCHS, validation_data = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with  convolutions\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),\n",
    "    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, None, 64)          523840    \n_________________________________________________________________\nconv1d (Conv1D)              (None, None, 128)         41088     \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 128)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_7 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 573,249\nTrainable params: 573,249\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n391/391 [==============================] - 212s 542ms/step - loss: 0.4577 - accuracy: 0.7747 - val_loss: 0.3146 - val_accuracy: 0.8746\nEpoch 2/2\n391/391 [==============================] - 154s 393ms/step - loss: 0.2245 - accuracy: 0.9158 - val_loss: 0.2993 - val_accuracy: 0.8777\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f418a8fa198>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model3.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model3.fit(train_dataset, epochs = NUM_EPOCHS, validation_data = test_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595574177807",
   "display_name": "Python 3.6.10 64-bit ('tensor': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}