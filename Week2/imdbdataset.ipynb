{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0\n"
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Split data into testing and training\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "train_labels = []\n",
    "testing_sentences = []\n",
    "testing_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[]\n"
    }
   ],
   "source": [
    "print(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels and sentences from tensors\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))\n",
    "  train_labels.append(l.numpy())\n",
    "  \n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(s.numpy().decode('utf8'))\n",
    "  testing_labels.append(l.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'>\n25000\n25000\n25000\n"
    }
   ],
   "source": [
    "print(type(training_sentences))\n",
    "print(len(training_sentences))\n",
    "print(len(train_labels))\n",
    "print(len(testing_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come. 1\nThis was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it. 0\n"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(testing_sentences[0], testing_labels[0])\n",
    "print(training_sentences[0], testing_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'>\n"
    }
   ],
   "source": [
    "print(type(testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to numpy arrays\n",
    "train_labels_final = np.array(train_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25000,)\n(25000,)\n"
    }
   ],
   "source": [
    "print(train_labels_final.shape)\n",
    "print(testing_labels_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the sentences - hyperparameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = \"post\"\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(25000, 120)\n25000\n(25000, 120)\n25000\n(25000, 120)\n"
    }
   ],
   "source": [
    "print(testing_padded.shape)\n",
    "print(len(testing_labels_final))\n",
    "print(padded.shape)\n",
    "print(len(train_labels_final))\n",
    "print(testing_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? this is the kind of film for a snowy sunday afternoon when the rest of the world can go ahead with its own business as you <OOV> into a big arm chair and <OOV> for a couple of hours wonderful performances from cher and nicolas cage as always gently row the plot along there are no <OOV> to cross no dangerous waters just a warm and witty <OOV> through new york life at its best a family film in every sense and one that deserves the praise it received\nThis is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n"
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[3]))\n",
    "print(training_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_8 (Embedding)      (None, 120, 16)           160000    \n_________________________________________________________________\nflatten_8 (Flatten)          (None, 1920)              0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 6)                 11526     \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 7         \n=================================================================\nTotal params: 171,533\nTrainable params: 171,533\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n782/782 [==============================] - 12s 16ms/step - loss: 0.0893 - accuracy: 0.9781 - val_loss: 0.4650 - val_accuracy: 0.8222\nEpoch 2/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.0230 - accuracy: 0.9969 - val_loss: 0.5353 - val_accuracy: 0.8245\nEpoch 3/10\n782/782 [==============================] - 13s 16ms/step - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.5992 - val_accuracy: 0.8238\nEpoch 4/10\n782/782 [==============================] - 13s 16ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 0.8250\nEpoch 5/10\n782/782 [==============================] - 12s 15ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7018 - val_accuracy: 0.8251\nEpoch 6/10\n782/782 [==============================] - 12s 16ms/step - loss: 4.6778e-04 - accuracy: 1.0000 - val_loss: 0.7412 - val_accuracy: 0.8255\nEpoch 7/10\n782/782 [==============================] - 13s 16ms/step - loss: 2.7250e-04 - accuracy: 1.0000 - val_loss: 0.7807 - val_accuracy: 0.8253\nEpoch 8/10\n782/782 [==============================] - 13s 16ms/step - loss: 1.5902e-04 - accuracy: 1.0000 - val_loss: 0.8169 - val_accuracy: 0.8258\nEpoch 9/10\n782/782 [==============================] - 12s 16ms/step - loss: 9.7296e-05 - accuracy: 1.0000 - val_loss: 0.8539 - val_accuracy: 0.8246\nEpoch 10/10\n782/782 [==============================] - 12s 15ms/step - loss: 6.1117e-05 - accuracy: 1.0000 - val_loss: 0.8880 - val_accuracy: 0.8255\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fdef4201908>"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "model.fit(padded,\n",
    " train_labels_final,\n",
    " epochs=num_epochs,\n",
    " validation_data = (testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595051728760",
   "display_name": "Python 3.6.10 64-bit ('tensor': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}